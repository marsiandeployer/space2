#!/usr/bin/env python3

import sys
import os
import tempfile
import yaml
sys.path.append('/workspaces/space2/hababru')

from src.backend.services.telegram_product_generator import TelegramProductGenerator, TelegramMessage
from unittest.mock import Mock

def test_semantic_debug():
    """–û—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ç–µ—Å—Ç –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏"""
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    temp_dir = tempfile.mkdtemp()
    
    # Mock LLM service
    mock_llm_service = Mock()
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = TelegramProductGenerator(
        llm_service=mock_llm_service,
        products_dir=temp_dir
    )
    
    # –°–æ–∑–¥–∞–µ–º YAML –ø—Ä–æ–¥—É–∫—Ç–∞ contract_analysis
    real_contract_yaml = """---
product_id: "contract_analysis"
name: "–ê–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å –ò–ò"
description: "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å –≤—ã—è–≤–ª–µ–Ω–∏–µ–º —Ä–∏—Å–∫–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"
version: "1.0"
category: "legal"
status: "active"

demo_data:
  key_features:
    - "–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–∞"
    - "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É"
    - "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"
  
  supported_formats:
    - "PDF"
    - "DOC"
    - "TXT"
  
  processing_time: "2-5 –º–∏–Ω—É—Ç"
  accuracy: "95%"

product_info:
  key_benefits:
    - "–≠–∫–æ–Ω–æ–º–∏—è –≤—Ä–µ–º–µ–Ω–∏ —é—Ä–∏—Å—Ç–æ–≤ –¥–æ 70%"
    - "–°–Ω–∏–∂–µ–Ω–∏–µ –ø—Ä–∞–≤–æ–≤—ã—Ö —Ä–∏—Å–∫–æ–≤"
    - "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä—É—Ç–∏–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫"
  
  target_audience:
    - "–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ñ–∏—Ä–º—ã"
    - "–ö–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ —é—Ä–∏—Å—Ç—ã"
    - "–ú–∞–ª—ã–π –∏ —Å—Ä–µ–¥–Ω–∏–π –±–∏–∑–Ω–µ—Å"

seo:
  keywords:
    - "–∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞"
    - "–ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–≥–æ–≤–æ—Ä–∞"
    - "—ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –¥–æ–≥–æ–≤–æ—Ä–∞"
    - "–∞–Ω–∞–ª–∏–∑ –¥–æ–≥–æ–≤–æ—Ä–∞ –ò–ò"
    - "–ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç—Ä–∞–∫—Ç–∞"
    - "—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"
"""
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
    existing_product_path = os.path.join(temp_dir, "contract_analysis.yaml")
    with open(existing_product_path, 'w', encoding='utf-8') as f:
        f.write(real_contract_yaml)
    
    # Telegram —Å–æ–æ–±—â–µ–Ω–∏–µ
    telegram_text = "üîç –ù–æ–≤—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞! –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –Ω–∞—Ö–æ–¥–∏–º —Ä–∏—Å–∫–∏."
    
    print("=== –û–¢–õ–ê–î–ö–ê –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ô –ü–†–û–í–ï–†–ö–ò ===")
    print(f"Telegram —Ç–µ–∫—Å—Ç: {telegram_text}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
    message_normalized = generator._normalize_text(telegram_text)
    print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è: {message_normalized}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã
    existing_products = generator._get_existing_products_with_data()
    print(f"–ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–¥—É–∫—Ç–æ–≤: {list(existing_products.keys())}")
    
    if "contract_analysis" in existing_products:
        product_data = existing_products["contract_analysis"]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞
        product_full_text = generator._extract_all_text_from_product(product_data)
        print(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞: {product_full_text[:200]}...")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞
        product_normalized = generator._normalize_text(product_full_text)
        print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–¥—É–∫—Ç–∞: {product_normalized[:200]}...")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarity = generator._calculate_text_similarity(message_normalized, product_normalized)
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞: {similarity:.3f}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        words1 = set(message_normalized.split())
        words2 = set(product_normalized.split())
        
        stop_words = {
            "–∏", "–≤", "–Ω–∞", "—Å", "–¥–ª—è", "–æ—Ç", "–ø–æ", "–¥–æ", "–∏–∑", "–∫", "–æ", "–æ–±", "–ø—Ä–∏", "–ø—Ä–æ", 
            "—á–µ—Ä–µ–∑", "–Ω–∞–¥", "–ø–æ–¥", "–º–µ–∂–¥—É", "–∞", "–Ω–æ", "–∏–ª–∏", "—á—Ç–æ", "–∫–∞–∫", "—ç—Ç–æ", "—Ç–æ", "–µ—Å–ª–∏",
            "—Ç–∞–∫", "—É–∂–µ", "–µ—â–µ", "–æ—á–µ–Ω—å", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—Ç–æ–º", "–∑–¥–µ—Å—å", "—Ç–∞–º", "–≤—Å–µ", "–≤–µ—Å—å"
        }
        
        words1_filtered = {word for word in words1 if len(word) > 2 and word not in stop_words}
        words2_filtered = {word for word in words2 if len(word) > 2 and word not in stop_words}
        
        intersection = words1_filtered.intersection(words2_filtered)
        
        print(f"–°–ª–æ–≤–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è: {words1_filtered}")
        print(f"–°–ª–æ–≤–∞ –∏–∑ –ø—Ä–æ–¥—É–∫—Ç–∞ (–ø–µ—Ä–≤—ã–µ 20): {list(words2_filtered)[:20]}")
        print(f"–û–±—â–∏–µ —Å–ª–æ–≤–∞: {intersection}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤: {len(intersection)}")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(words1_filtered.union(words2_filtered))}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –¥—É–±–ª—å
    duplicate_id = generator._check_semantic_duplicate(TelegramMessage(
        message_id=999,
        text=telegram_text,
        date="2025-07-03T16:00:00Z"
    ))
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª—è: {duplicate_id}")
    
    # –û—á–∏—Å—Ç–∫–∞
    import shutil
    shutil.rmtree(temp_dir)

if __name__ == "__main__":
    test_semantic_debug()
