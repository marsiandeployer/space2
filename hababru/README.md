# Сервис для проверки договоров

## Описание Проекта

Данный сервис предназначен для автоматизированного анализа юридических договоров. Пользователь загружает текст договора (в форматах PDF или DOC), после чего система асинхронно анализирует каждый смысловой пункт/абзац. Прогресс анализа отображается в реальном времени. По завершении анализа, при наведении на элемент текста справа отображаются потенциальные риски, рекомендации по улучшению формулировок и информация о том, как данная часть текста соотносится с другими разделами договора.

Сервис использует универсальный LLM-коннектор, который может работать как с DeepSeek API, так и с OpenAI API (в зависимости от настроек в `.env`), для анализа текста и кэширует результаты анализа для повышения производительности и возможности отслеживания прогресса.

Проект также включает в себя набор SEO-оптимизированных HTML-страниц, каждая из которых посвящена определенному ключевому запросу, связанному с юридическим анализом документов. Контент этих страниц будет обогащаться данными из Яндекс.Вордстата.

## Технологический Стек

*   **Бэкенд**: Python (Flask)
    *   Парсинг документов: `pdfminer.six`, `python-docx`
    *   Взаимодействие с API: `requests`
    *   Кэширование: Файловая система (JSON)
    *   Шаблонизатор: Jinja2
*   **Фронтенд**: Vanilla JavaScript
*   **API для анализа**: Универсальный LLM-коннектор (DeepSeek/OpenAI)
*   **Хранение контента**: Markdown с YAML Front Matter
*   **Хранение контента**: Markdown с YAML Front Matter

## Структура Проекта

```
hababru/
├── .env                      # Переменные окружения (API ключи, токены)
├── .gitignore                # Игнорируемые файлы и директории для Git
├── .pytest_cache/            # Кэш pytest
├── .wakatime-project         # Файл конфигурации Wakatime
├── .wata                     # Вспомогательный файл (возможно, для внутренних нужд)
├── README.md                 # Описание проекта, структура, план работы, инструкции
├── TGStat API.postman_collection.json # Коллекция Postman для работы с TGStat API
├── content/                  # Исходные файлы контента
│   ├── __init__.py           # Инициализация Python-пакета
│   ├── llm_results/          # Результаты работы LLM-промптов
│   │   └── *.txt             # Файлы с результатами LLM-промптов
│   ├── seo_pages/            # Директория для SEO-страниц в формате .md
│   │   ├── __init__.py       # Инициализация Python-пакета
│   │   ├── [slug]/           # Директория для каждой SEO-страницы (например, arendy, dareniya)
│   │   │   ├── generated_contract.txt # Сгенерированный текст договора для страницы
│   │   │   └── source.md     # Метаданные и основной текст страницы
│   │   └── ...               # Другие SEO-страницы (по одной на каждый ключ)
│   └── seo_prompts/          # Шаблоны промптов для генерации SEO-контента
│       └── *.txt             # Файлы с шаблонами промптов
├── data/                     # Хранилище данных
│   ├── __init__.py           # Инициализация Python-пакета
│   ├── cache/                # Кэш договоров и анализов (например, JSON файлы)
│   │   └── __init__.py       # Инициализация Python-пакета
│   ├── uploads/              # Временно загруженные файлы
│   │   └── __init__.py       # Инициализация Python-пакета
│   └── sample_contracts/     # Примеры договоров
│       ├── __init__.py       # Инициализация Python-пакета
│       └── default_nda.txt   # Пример договора по умолчанию
├── deepseek_api_docs.md      # Документация по DeepSeek API
├── dubna.docx                # Пример DOCX файла (возможно, для тестирования парсинга)
├── public/                   # Статические ресурсы и скомпилированный фронтенд
│   ├── css/                  # Стили CSS
│   │   └── style.css         # Основной файл стилей
│   ├── favicon.ico           # Иконка сайта
│   ├── js/                   # Клиентские скрипты JavaScript
│   │   ├── app.js            # Основная логика фронтенда
│   │   └── seo_admin.js      # Скрипты для страницы администрирования SEO
│   └── robots.txt            # Файл для поисковых роботов
├── pytest.ini                # Конфигурационный файл pytest
├── pytest_output.txt         # Вывод результатов выполнения pytest
├── requirements.txt          # Зависимости Python
├── semantix.txt              # Файл с семантическими запросами (возможно, для SEO)
├── src/                      # Исходный код
│   ├── backend/              # Логика бэкенда (Python)
│   │   ├── __init__.py       # Инициализация Python-пакета
│   │   ├── __pycache__/      # Кэш скомпилированных Python-файлов
│   │   ├── api/              # API эндпоинты
│   │   │   ├── __init__.py   # Инициализация Python-пакета
│   │   │   ├── __pycache__/  # Кэш скомпилированных Python-файлов
│   │   │   └── v1/
│   │   │       ├── __init__.py # Инициализация Python-пакета
│   │   │       ├── __pycache__/ # Кэш скомпилированных Python-файлов
│   │   │       ├── contract_analyzer.py # Эндпоинт для анализа договоров
│   │   │       └── seo_tools.py      # Эндпоинты для администрирования SEO
│   │   ├── cli/              # Скрипты командной строки
│   │   │   ├── __init__.py   # Инициализация Python-пакета
│   │   │   └── generate_seo_page.py # Скрипт для генерации SEO-страниц
│   │   ├── main.py           # Точка входа бэкенд-приложения
│   │   ├── services/         # Бизнес-логика
│   │   │   ├── __init__.py   # Инициализация Python-пакета
│   │   │   ├── __pycache__/  # Кэш скомпилированных Python-файлов
│   │   │   ├── cache_service.py    # Логика кэширования
│   │   │   ├── content_generation_service.py # Сервис для генерации контента
│   │   │   ├── llm_service.py      # Универсальный LLM-коннектор
│   │   │   ├── parsing_service.py  # Парсинг PDF/DOC
│   │   │   ├── seo_prompt_service.py # Сервис для работы с промптами SEO
│   │   │   └── seo_service.py      # Сервис для работы с SEO-страницами
│   │   ├── templates/        # HTML-шаблоны Jinja2
│   │   │   ├── index_template.html # Единый шаблон для всех страниц
│   │   │   └── seo_admin_template.html # Шаблон для страницы администрирования SEO
│   │   └── utils/            # Вспомогательные функции
│   │       └── __init__.py   # Инициализация Python-пакета
│   ├── data/                 # Директория для данных (например, загруженных файлов)
│   │   └── uploads/          # Временно загруженные файлы
│   ├── frontend/             # Исходный код фронтенда
│   │   ├── __init__.py       # Инициализация Python-пакета
│   │   ├── components/       # UI компоненты
│   │   │   └── __init__.py   # Инициализация Python-пакета
│   │   └── services/         # Сервисы фронтенда (вызовы API)
│   │       └── __init__.py   # Инициализация Python-пакета
│   └── shared/               # Общий код для фронтенда и бэкенда
│       └── __init__.py       # Инициализация Python-пакета
├── tests/                    # Автоматизированные тесты
│   ├── test_api.py           # Тесты для API эндпоинтов
│   ├── test_cache_service.py # Тесты для сервиса кэширования
│   ├── test_llm_service.py   # Тесты для LLM-сервиса
│   ├── test_parsing_service.py # Тесты для сервиса парсинга
│   ├── test_seo_admin.py     # Тесты для администрирования SEO
│   ├── test_seo_prompt_api.py # Тесты для API запуска промптов SEO
│   └── test_seo_service.py   # Тесты для SEO-сервиса
└── tree.txt                  # Файл с древовидной структурой проекта
```

## План Работы (Выполнено)

1.  **Инициализация Проекта:**
    *   Создана корневая директория `hababru/`.
    *   Создана базовая структура папок внутри `hababru/`.
    *   Создан файл `hababru/.env` с `DEEPSEEK_API_KEY`, `YANDEX_CLIENT_ID`, `YANDEX_CLIENT_SECRET`, `YANDEX_REDIRECT_URI`.
    *   Создан файл `hababru/README.md` с этим подробным описанием.
    *   Подготовлен файл `data/sample_contracts/default_nda.txt` с примером договора.
    *   Созданы все необходимые `__init__.py` файлы для корректной работы пакетов Python.
    *   Установлены зависимости из `requirements.txt`.

2.  **Разработка Бэкенда (Python с Flask):**
    *   Настроен Flask-фреймворк и реализован `src/backend/main.py` как точка входа.
    *   **Обновлено**: Рефакторинг `src/backend/main.py` для использования функции `create_app()` для инициализации Flask приложения. Это позволяет создавать чистые экземпляры приложения для каждого тестового запуска.
    *   Реализован `src/backend/services/parsing_service.py` для парсинга PDF/DOCX и сегментации текста на пункты/абзацы.
    *   Реализован `src/backend/services/llm_service.py` как универсальный LLM-коннектор, поддерживающий DeepSeek и OpenAI API.
    *   **Обновлено**: Реализован `src/backend/services/cache_service.py` для кэширования результатов анализа и управления статусом асинхронных задач.
    *   **Обновлено**: Реализован `src/backend/api/v1/contract_analyzer.py` с API-эндпоинтами `/upload_contract`, `/start_analysis` (для запуска асинхронного анализа), `/get_analysis_status/<task_id>` (для получения прогресса) и `/get_sample_contract`.
    *   **Обновлено**: Рефакторинг `src/backend/api/v1/seo_tools.py` для использования функции `create_seo_tools_blueprint()`, что позволяет избежать конфликтов при регистрации эндпоинтов в тестах.
    *   Blueprint `contract_analyzer_bp` зарегистрирован в `main.py`.
    *   **Обновлено**: Главная страница и SEO-страницы теперь рендерятся с использованием единого шаблона `src/backend/templates/index_template.html`.

3.  **Разработка Фронтенда (Vanilla JS):**
    *   Статический файл `public/index.html` удален. Главная страница теперь рендерится бэкендом.
    *   Создан `public/css/style.css` для общих стилей.
    *   **Обновлено**: Создан `public/js/app.js` с единой логикой для главной и SEO-страниц:
        *   Добавлен `public/favicon.ico`.
        *   Рандомизация реквизитов примера договора.
        *   Загрузка примера договора через API `/api/v1/get_sample_contract`.
        *   Отображение текста договора по пунктам/абзацам.
        *   Базовая логика для отображения панели анализа при наведении.
        *   Обработчик для загрузки пользовательских файлов и отправки их на анализ.
        *   **Новое**: Реализован асинхронный запуск анализа через `/api/v1/start_analysis`.
        *   **Новое**: Добавлена логика периодического опроса `/api/v1/get_analysis_status/<task_id>` для отображения прогресса анализа (количество обработанных пунктов/абзацев и процент выполнения).
        *   **Новое**: Добавлены элементы UI (текстовое поле и прогресс-бар) для визуализации прогресса.
    *   **Новое**: На главную страницу (теперь через `index_template.html`) добавлен текст о сервисе и ссылки на существующие SEO-страницы договоров.

5.  **Унификация шаблонов и стилей:**
    *   Удален шаблон `src/backend/templates/seo_page_template.html`.
    *   Все страницы теперь используют единый шаблон `src/backend/templates/index_template.html`.
    *   Стили унифицированы в `public/css/style.css`.
    *   JavaScript-логика унифицирована в `public/js/app.js`.

6.  **Тестирование и Отладка:**
    *   Проверена базовая структура проекта и установка зависимостей.
    *   Проверена работа Flask-приложения и обслуживание статических файлов.
    *   Проверена загрузка примера договора через API.
    *   **Обновлено**: Проверена асинхронная работа анализа и отображение прогресса на фронтенде.
    *   **Обновлено**: Автотесты были рефакторингованы для использования `create_app()` и передачи моков сервисов, что решило проблемы с перезаписью эндпоинтов и некорректным мокированием.
    *   **Обновлено**: Исправлена ошибка `TypeError: a bytes-like object is required, not 'str'` в `test_get_sample_contract` путем изменения `read_data` в `mock_open` на байтовую строку.
    *   **Обновлено**: Оптимизировано выполнение `test_seo_page_ipotechnyh_dogovorov_content_display` путем мокирования `analysis_results_raw` с ограниченным количеством пунктов, чтобы избежать долгого анализа.

## Текущий Статус и Известные Проблемы

Все файлы проекта, включая бэкенд-сервисы, API-эндпоинты и базовый фронтенд, созданы согласно плану. Асинхронный анализ и отображение прогресса реализованы.

### Обновления по передаче данных на SEO-страницы

*   **Улучшена передача данных на SEO-страницы**: Передача данных `appConfig` из бэкенда на фронтенд для SEO-страниц была переработана для повышения надежности. Теперь данные сериализуются в JSON на стороне Python (`src/backend/services/seo_service.py`) и передаются в скрытый `div` в `src/backend/templates/index_template.html`. Фронтенд (`public/js/app.js`) считывает эти данные из `textContent` этого `div` и парсит их как JSON. Это устраняет проблемы с экранированием символов и обеспечивает корректную инициализацию `window.appConfig`.
*   **Обновлены автотесты**: Соответствующие автотесты (`tests/test_api.py`) были обновлены для проверки нового механизма передачи данных, включая извлечение и парсинг JSON из HTML-ответа.

### Исправленные проблемы

*   **Исправлен `test_get_llm_models`**: Проблема, из-за которой тест `test_get_llm_models` завершался неудачей (возвращал `None` вместо списка моделей), была устранена путем добавления оператора `return unique_models` в конце функции `get_available_models` в `src/backend/services/llm_service.py`.
*   **Удален дублирующийся код в `llm_service.py`**: Из `src/backend/services/llm_service.py` был удален дублирующийся блок кода, что повысило читаемость и поддерживаемость файла.
*   **Исправлен `test_seo_page_content_display`**: Проблема с отображением контента на SEO-страницах, которая приводила к ошибке в `test_seo_page_content_display`, была решена путем изменения логики инициализации `window.appConfig` в `src/backend/templates/index_template.html`. Теперь данные из скрытого `div` парсятся в `window.appConfig` до загрузки `app.js`, что гарантирует их доступность. Также была удалена дублирующая логика парсинга `app-config-data` и дублирующая функция `loadTestContractAndAnalyze` из `public/js/app.js`.

## Авто тесты

Для запуска всех тестов используйте `pytest` в корневой директории `hababru`.
```bash
cd hababru && pytest
```
Для запуска конкретных тестов, например, только тех, которые ранее падали:
```bash
cd hababru && pytest tests/test_api.py::test_seo_page tests/test_api.py::test_get_sample_contract tests/test_api.py::test_upload_contract tests/test_api.py::test_seo_page_content_display tests/test_api.py::test_seo_page_ipotechnyh_dogovorov_content_display tests/test_seo_prompt_api.py::test_run_openai_prompt_success
```

## Примеры договоров для SEO-страниц

Примеры договоров, специфичные для каждой SEO-оптимизированной страницы, теперь размещаются в соответствующих поддиректориях внутри `hababru/content/seo_pages/`.
Название поддиректории должно соответствовать ключевому слову или URL SEO-страницы. Например:
*   Для SEO-страницы `analiz-dogovora-arendy` примеры договоров размещаются в `hababru/content/seo_pages/analiz-dogovora-arendy/`. (при тестах сео страниц используй эту страницу по умолчанию)
*   Для SEO-страницы `analiz-dogovora-postavki` примеры договоров размещаются в `hababru/content/seo_pages/analiz-dogovora-postavki/`.

Это позволит в будущем реализовать логику загрузки релевантных примеров для каждой конкретной SEO-страницы.

## Инструкции по Запуску (Обновлено)

1.  **Клонирование репозитория:**
    ```bash
    git clone <URL_репозитория>
    cd hababru
    ```

2.  **Установка зависимостей и активация виртуального окружения:**
    ```bash
    python3.12 -m venv venv
    source venv/bin/activate  # Для Linux/macOS
    # или
    venv\Scripts\activate     # Для Windows
    pip install -r requirements.txt
    ```

3.  **Настройка переменных окружения:**
    *   Создайте файл `.env` в корневой директории проекта `hababru/`.
    *   Добавьте следующие переменные:
        ```
        DEEPSEEK_API_KEY=ВАШ_DEEPSEEK_API_KEY
        OPENAI_KEY=
        ```

    *   **Примечание**: Агент не будет изменять содержимое файла `.env`.

4.  **Запуск бэкенда:**
    *   **Убедитесь, что виртуальное окружение активировано:**
        ```bash
        source venv/bin/activate  # Для Linux/macOS
        # или
        venv\Scripts\activate     # Для Windows
        ```
    
    *   **Запуск через PM2 (рекомендуется для production):**
        Запустите приложение с помощью PM2:
            ```bash
            pm2 stop "habab_space2" || true
            pm2 delete "habab_space2" || true
            pm2 flush "habab_space2" || true
            pm2 start "cd hababru && python3 -m src.backend.main" --name "habab_space2"
            ```
            
            # для просмотра логов (таймаут нужен т.к. cline не продолжает работу пока не выйти в баш обратно из команды)
            timeout 10 pm2 logs habab_space2
            ```

5.  **Доступ к сервису:**
    *   Основной интерфейс будет доступен по адресу `http://127.0.0.1:5001/` (или другой порт, указанный в конфигурации Flask).
    *   SEO-страницы будут доступны по соответствующим URL (например, `http://127.0.0.1:5001/analiz-dogovora-arendy`).

## Пример Договора по Умолчанию

Текст предоставленного вами "Соглашения о защите и неразглашении информации" сохранен в `data/sample_contracts/default_nda.txt`. При отображении на фронтенде его реквизиты (ФИО, ИНН, адреса, названия компаний и т.д.) будут заменены на случайные данные для демонстрации.

---
**Примечание:** Для полноценной работы с API Яндекс.Вордстата требуется ручное получение OAuth-токена и подача заявка на доступ к API через поддержку Яндекс.Директа.

## Обработка ошибок LLM API

В процессе работы сервиса могут возникать ошибки при взаимодействии с выбранным LLM API (DeepSeek или OpenAI), например, `Read timed out`. Для повышения стабильности работы рекомендуется рассмотреть следующие улучшения в `src/backend/services/llm_service.py`:
*   **Механизм повторных попыток (Retry Logic)**: Реализовать автоматические повторные попытки запросов к LLM API с экспоненциальной задержкой (exponential backoff) в случае временных ошибок (например, таймаутов или ошибок 5xx). Это поможет справиться с нестабильностью сети или временной недоступностью API.
*   **Более информативные сообщения об ошибках**: Улучшить логирование ошибок, чтобы они содержали больше контекста, например, полный текст ошибки от API, статус-коды HTTP и время, прошедшее до таймаута. Это упростит отладку и мониторинг.
*   **Конфигурируемые таймауты**: Сделать таймауты для запросов к LLM API конфигурируемыми, чтобы их можно было легко настраивать в зависимости от условий сети и ожидаемого времени ответа API.

## Генерация SEO-страниц

Для создания новой SEO-страницы или обновления существующей используйте скрипт `generate_seo_page.py`. Этот скрипт автоматически создает необходимую структуру директорий и генерирует контент (`source.md` и `generated_contract.txt`) на основе заданного ключевого слова.

**Использование:**

```bash
python src/backend/cli/generate_seo_page.py --keyword "ВАШЕ КЛЮЧЕВОЕ СЛОВО"
```

**Пример:**

Для создания страницы "анализ договора аренды недолго помещения" выполните:

```bash
python src/backend/cli/generate_seo_page.py --keyword "анализ договора аренды нежилого помещения"
```

Скрипт создаст директорию `content/seo_pages/analiz-dogovora-arendy-nedolgo-pomeshcheniya/` и заполнит ее необходимыми файлами.


## План по Автоматической Генерации SEO-страниц (Обновленный)

Эта секция описывает план по созданию системы для полуавтоматической генерации SEO-оптимизированных страниц на основе ключевых слов.

### Ключевые изменения в подходе:

1.  **Анализ Договора**:
    *   Скрипт генерации SEO-страницы (`src/backend/cli/generate_seo_page.py` через `src/backend/services/content_generation_service.py`) **не будет** сам запускать или сохранять результаты анализа сгенерированного договора.
    *   Он только сгенерирует текст договора и сохранит его в `content/seo_pages/[slug]/generated_contract.txt`.
    *   Когда пользователь впервые заходит на SEO-страницу (например, `/анализ-договора-аренды`), существующая система (которая обрабатывает `?test=файл.pdf`) автоматически подхватит `generated_contract.txt` для этой страницы и выполнит анализ "на лету". Результаты анализа будут отображаться, но не будут предварительно сохраняться в `source.md` генератором.

2.  **Яндекс.Вордстат и Фаллбэк**:
    *   В `.env` файле будет флаг `USE_YANDEX_WORDSTAT` (по умолчанию `false`, если отсутствует).
    *   **Если `USE_YANDEX_WORDSTAT=true`**:
        *   `ContentGenerationService` попытается получить `meta_keywords` и `related_keywords` из Яндекс.Вордстат.
        *   Если Вордстат недоступен, возвращает ошибку, или если флаг `USE_YANDEX_WORDSTAT` установлен в `false` изначально, то:
            *   `meta_keywords` и `related_keywords` будут сгенерированы с помощью DeepSeek на основе основного ключевого слова.
    *   **Если `USE_YANDEX_WORDSTAT=false`**:
        *   `meta_keywords` и `related_keywords` сразу генерируются DeepSeek.

### Обновленная структура `content/seo_pages/[slug]/source.md`:

```yaml
---
title: "Анализ договора аренды" # Генерируется из основного ключевого слова
meta_keywords: ["ключ1", "ключ2"] # Из Яндекс.Вордстат или сгенерированы DeepSeek
meta_description: "Сгенерированное описание..." # Сгенегировано DeepSeek
related_keywords: ["связанный ключ1", "связанный ключ2"] # Из Яндекс.Вордстат или сгенегированы DeepSeek
contract_file: "generated_contract.txt" # Путь к файлу сгенерированного текста договора
main_keyword: "анализ договора аренды" # Исходное ключевое слово для страницы
---
# Сгенерированный основной текст страницы (под формой)
# Этот текст также генерируется DeepSeek.
```

### Обновленный процесс в `src/backend/services/content_generation_service.py`:

1.  **Генерация текста договора**: На основе `main_keyword` с помощью LLM. Сохраняется в `content/seo_pages/[slug]/generated_contract.txt`.
2.  **Получение/Генерация ключевых слов (`meta_keywords`, `related_keywords`)**:
    *   Проверяется флаг `USE_YANDEX_WORDSTAT` в `.env` (по умолчанию `false`).
    *   Если `true` и Вордстат доступен: используются данные из `yandex_wordstat_service.py`.
    *   В противном случае (флаг `false` или Вордстат недоступен/ошибка): ключевые слова генерируются LLM.
3.  **Генерация `meta_description`**: С помощью LLM (на основе `main_keyword` и, возможно, полученных/сгенерированных `meta_keywords`).
4.  **Генерация основного текста страницы**: С помощью LLM (на основе `main_keyword`).
5.  **Сборка `source.md`**: Формируется файл `source.md` с полями: `title` (из `main_keyword`), `meta_keywords`, `meta_description`, `related_keywords`, `contract_file` (путь), `main_keyword` и сгенерированный основной текст страницы.### Взаимодействие при отображении SEO-страницы:

**Серверная часть (`src/backend/main.py`, `src/backend/services/seo_service.py`):**

1.  Пользователь запрашивает `/slug_страницы`.
2.  `seo_service.py` загружает:
    *   Метаданные и основной текст из `content/seo_pages/[slug_страницы]/source.md`.
    *   Текст сгенерированного договора из `content/seo_pages/[slug_страницы]/generated_contract.txt`.
3.  Если текст договора найден, `seo_service.py` инициирует "анализ на лету" для этого договора, используя `llm_service` и `cache_service`. Результаты анализа (сегментация и анализ каждого абзаца) кэшируются гранулированно.
4.  Все собранные данные (метаданные, основной текст страницы, текст договора и результаты анализа) передаются в единый HTML-шаблон `src/backend/templates/index_template.html`.
5.  В шаблоне эти данные встраиваются в скрытый `div` элемент (`#seo-data`) в виде `data-` атрибутов, содержащих JSON-строки.

**Клиентская часть (`public/js/app.js`):**

1.  При загрузке страницы `app.js` определяет, является ли текущая страница SEO-страницей, проверяя наличие и значение `data-is-seo-page` атрибута в `#seo-data` div.
2.  Если это SEO-страница, `app.js` считывает `data-contract-text-raw` и `data-analysis-results-raw` из `#seo-data` div.
3.  Полученные JSON-строки парсятся в JavaScript-объекты.
4.  Текст договора отображается на странице, и запускается процесс отображения анализа. Если анализ уже был выполнен и закэширован на сервере, он отображается мгновенно. В противном случае, запускается асинхронный опрос статуса анализа, и прогресс отображается в реальном времени.
5.  Таким образом, SEO-страницы динамически загружают и анализируют свой специфический контент, обеспечивая актуальность данных и интерактивность.

### Визуализация обновленного процесса генерации:
```mermaid
graph TD
    A[Запуск CLI: generate_seo_page --keyword "Ключевое слово"] --> B(Создание директории content/seo_pages/[slug]);
    B --> C(ContentGenerationService);

    subgraph ContentGenerationService
        C --> D{1. Генерация текста договора (LLM)};
        D --> E[generated_contract.txt];
        C --> I_LLM_KEYS{Генерация ключей (LLM)};
        I_LLM_KEYS --> J_LLM_KEYS_RES[meta_keywords, related_keywords (из LLM)];
        J_LLM_KEYS_RES --> K_MERGED_KEYS((Собранные ключи));
        K_MERGED_KEYS --> L{3. Генерация Meta Description (LLM)};
        L --> M[meta_description_text];
        K_MERGED_KEYS --> N{4. Генерация основного текста страницы (LLM)};
        N --> O[page_text_content];
        P[Все данные + main_keyword] --> Q_BUILD{5. Сборка source.md};
        Q_BUILD --> R_MD[source.md в content/seo_pages/[slug]/];
    end

    S[HTTP Запрос: GET /[slug]] --> T_FLASK(Flask Route);
    T_FLASK --> U_SEO_SVC(SeoService);
    U_SEO_SVC --> V_READ_FILES{Чтение source.md и generated_contract.txt};
    V_READ_FILES --> W_CONTRACT_DATA[Текст договора из generated_contract.txt];
    V_READ_FILES --> Z_METADATA[Метаданные и текст из source.md];
    W_CONTRACT_DATA --> X_ANALYSIS_SYSTEM{Передача текста в СУЩЕСТВУЮЩУЮ систему анализа};
    X_ANALYSIS_SYSTEM --> Y_LIVE_ANALYSIS[Результат анализа (на лету)];
    Y_LIVE_ANALYSIS & Z_METADATA --> AA_TEMPLATE{Передача всех данных в HTML-шаблон};
    AA_TEMPLATE --> AB_HTML[HTML Ответ];
